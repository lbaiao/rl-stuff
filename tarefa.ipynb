{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bita14175d2031447f79880e35aa3dfad5d",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tarefa\n",
    "__________\n",
    "Resolver o problema do rato utilizando Q-Table. Você deve programar o ambiente e o algoritmo de Reinforcement Learning.\n",
    "\n",
    "![alt text](./rato.png \"Rato\")\n",
    "\n",
    "Sobre o ambiente:\n",
    "  * O episódio deve terminar quando o rato alcançar a pilha de queijos ou tomar o veneno.\n",
    "  * O objetivo é fazer com que o rato pegue todos os queijos do mapa sem tomar o veneno.\n",
    "  * As ações devem ser mover o rato, em 1 casa, para cima, baixo, esquerda e direita.\n",
    "  * O rato está confinado no espaço de 6 casas, conforme a imagem abaixo.\n",
    "\n",
    "Dicas:\n",
    "  * É possível completar toda a tarefa utilizando apenas numpy.\n",
    "  * Ler sobre Q-Learning na referência \\[1\\].\n",
    "  * Representar o mapa como uma matriz.\n",
    "\n",
    "Tabela Q sugerida:\n",
    "\n",
    "![alt text](./tabela.png \"Q-Table\")\n",
    "\n",
    "Referências:\n",
    "\n",
    "1. PALANISAMY, Praveen. Hands-On Intelligent Agents with OpenAI Gym: Your guide to   developing AI agents using deep reinforcement learning. Packt Publishing Ltd, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrega\n",
    "___________\n",
    "\n",
    "Prazo: **22/02**\n",
    "\n",
    "Enviar para **lbpires@latam.stefanini.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código\n",
    "____\n",
    "\n",
    "### Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingParameters:\n",
    "    def __init__(self, max_episodes: int, steps_per_episode: int):  \n",
    "        self.max_episodes = int(max_episodes)\n",
    "        self.steps_per_episode = int(steps_per_episode)\n",
    "\n",
    "\n",
    "class AgentParameters:\n",
    "    def __init__(self, epsilon_min, epsilon_decay, start_epsilon):\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.start_epsilon = start_epsilon\n",
    "\n",
    "\n",
    "class LearningParameters:\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QTable:\n",
    "    def __init__(self, num_states: int, num_actions: int, params: LearningParameters):\n",
    "        self.table = np.random.rand(num_states, num_actions)\n",
    "        self.gamma = params.gamma\n",
    "        self.alpha = params.alpha\n",
    "\n",
    "    # calculates Q-table values\n",
    "    def learn(self, obs, action_index, reward, next_obs):\n",
    "        deltaQ = reward + self.gamma*np.max(self.table[next_obs]) - self.table[obs, action_index]\n",
    "        self.table[obs, action_index] = self.table[obs,action_index] + self.alpha*deltaQ"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params: AgentParameters, actions_shape):\n",
    "        self.epsilon_min = params.epsilon_min\n",
    "        self.epsilon_decay = params.epsilon_decay\n",
    "        self.epsilon = params.start_epsilon\n",
    "        self.actions = [i for i in range(actions_shape)]\n",
    "\n",
    "\n",
    "    def set_q_table(self, q_table: QTable):\n",
    "        self.q_table = q_table\n",
    "\n",
    "\n",
    "    def set_actions(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        if np.random.random() > self.epsilon:\n",
    "            action_index = np.argmax(self.q_table.table[obs,:])\n",
    "            self.action = self.actions[action_index]            \n",
    "        else:\n",
    "            action = np.random.choice(self.actions)\n",
    "            self.action = action\n",
    "        return self.action                \n",
    "        \n",
    "\n",
    "    def set_action(self, action_index: int):\n",
    "        self.action_index = action_index\n",
    "        self.action = self.actions[action_index]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Código do ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Função de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Função de teste, exibindo o correto funcionamento do algoritmo."
   ]
  }
 ]
}